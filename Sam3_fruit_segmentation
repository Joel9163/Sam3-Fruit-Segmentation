import torch
import torchvision
import sys
import os
import numpy as np
from PIL import Image
from typing import List
from sam3.train.data.collator import collate_fn_api as collate
from sam3.model.utils.misc import copy_data_to_device
from sam3.model_builder import build_sam3_image_model
from sam3.train.transforms.basic_for_api import ComposeAPI, RandomResizeAPI, ToTensorAPI, NormalizeAPI
from sam3.eval.postprocessors import PostProcessImage
from sam3.visualization_utils import plot_results
from sam3.train.data.sam3_image_dataset import InferenceMetadata, FindQueryLoaded, Image as SAMImage, Datapoint


CONFIG = {
    "paths": {
        "bpe_vocab": "/pub0/joel/fruit_segmentation/sam3/assets/bpe_simple_vocab_16e6.txt.gz",
        "output_dir": "/pub0/joel/fruit_segmentation/batch_results/",
        "reference_image": "/pub0/joel/fruit_segmentation/orange2_images/orange3_highres.png",
        "reference_box_xywh": [240, 190.0, 160, 200.0],
        "batch_images": [
            "/pub0/joel/fruit_segmentation/orange2_images/orange3_highres.png",
            "/pub0/joel/fruit_segmentation/orange_images/orange1.jpg"
        ]
    },
    "model_settings": {
        "prompt": "orange segment",
        "input_size": 1008,
        "detection_threshold": 0.4,
        "device": "cuda" if torch.cuda.is_available() else "cpu",
    },
    "filtering_thresholds": {
        "color_dist_limit": 80.0,
        "min_conf_for_size_baseline": 0.25,
        "size_multiplier_min": 0.1,
        "size_multiplier_max": 4.0
    }
}


torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
device = CONFIG["model_settings"]["device"]


GLOBAL_COUNTER = 1

def create_empty_datapoint():
    return Datapoint(find_queries=[], images=[])

def set_image(datapoint, pil_image):
    w, h = pil_image.size
    datapoint.images = [SAMImage(data=pil_image, objects=[], size=[h, w])]

def add_text_prompt(datapoint, text_query):
    global GLOBAL_COUNTER
    w, h = datapoint.images[0].size
    datapoint.find_queries.append(
        FindQueryLoaded(
            query_text=text_query,
            image_id=0,
            object_ids_output=[],
            is_exhaustive=True,
            query_processing_order=0,
            inference_metadata=InferenceMetadata(
                coco_image_id=GLOBAL_COUNTER,
                original_image_id=GLOBAL_COUNTER,
                original_category_id=1,
                original_size=[w, h],
                object_id=0,
                frame_index=0,
            )
        )
    )
    GLOBAL_COUNTER += 1
    return GLOBAL_COUNTER - 1

def get_reference_features(image_path, box_xywh):
    img = Image.open(image_path).convert("RGB")
    x, y, w, h = [int(v) for v in box_xywh]
    crop = img.crop((x, y, x+w, y+h))
    mean_color = np.array(crop).mean(axis=(0, 1)) 
    return {"color": mean_color}

def check_color_match(reference, mask_np, original_image_np):
    if mask_np.ndim == 3: mask_np = mask_np.squeeze()
    masked_pixels = original_image_np[mask_np > 0]
    if masked_pixels.size == 0: return False
    mean_color = masked_pixels.mean(axis=0)
    color_dist = np.linalg.norm(reference['color'] - mean_color)
    return color_dist < CONFIG["filtering_thresholds"]["color_dist_limit"]


print("Loading Model...")
model = build_sam3_image_model(bpe_path=CONFIG["paths"]["bpe_vocab"]).to(device)

transform = ComposeAPI(transforms=[
    RandomResizeAPI(sizes=CONFIG["model_settings"]["input_size"], max_size=CONFIG["model_settings"]["input_size"], square=True, consistent_transform=False),
    ToTensorAPI(),
    NormalizeAPI(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
])

postprocessor = PostProcessImage(
    max_dets_per_img=-1,
    iou_type="segm",
    use_original_sizes_box=True,
    use_original_sizes_mask=True,
    convert_mask_to_rle=False,
    detection_threshold=CONFIG["model_settings"]["detection_threshold"], 
    to_cpu=True,
)

print("Learning Reference Color...")
ref_features = get_reference_features(CONFIG["paths"]["reference_image"], CONFIG["paths"]["reference_box_xywh"])

print("Building Batch...")
datapoints, query_map = [], []
for path in CONFIG["paths"]["batch_images"]:
    if not os.path.exists(path): continue
    img = Image.open(path).convert("RGB")
    dp = create_empty_datapoint()
    set_image(dp, img)
    qid = add_text_prompt(dp, CONFIG["model_settings"]["prompt"])
    datapoints.append(transform(dp))
    query_map.append({"id": qid, "path": path, "original_image": np.array(img)})

print("Running Inference...")
dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
with torch.autocast("cuda", dtype=dtype), torch.inference_mode():
    batch = collate(datapoints, dict_key="dummy")["dummy"]
    batch = copy_data_to_device(batch, device, non_blocking=True)
    output = model(batch)
    processed_results = postprocessor.process_results(output, batch.find_metadatas)

print("Adaptive Filtering Results...")
os.makedirs(CONFIG["paths"]["output_dir"], exist_ok=True)

for item in query_map:
    qid = item["id"]
    if qid not in processed_results: continue
    res = processed_results[qid]
    
    masks, scores, boxes = res["masks"], res["scores"], res["boxes"]
    if torch.is_tensor(masks): masks = masks.float().cpu().numpy()
    if torch.is_tensor(scores): scores = scores.float().cpu().numpy()
    if torch.is_tensor(boxes): boxes = boxes.float().cpu().numpy()

    # Color Pass
    candidates = []
    for i, mask in enumerate(masks):
        if check_color_match(ref_features, mask, item["original_image"]):
             candidates.append({"index": i, "area": np.sum(mask > 0), "score": scores[i]})
    
    if not candidates: continue

    # Local Size Average
    high_conf_areas = [c["area"] for c in candidates if c["score"] > CONFIG["filtering_thresholds"]["min_conf_for_size_baseline"]]
    local_median_area = np.median(high_conf_areas) if high_conf_areas else np.median([c["area"] for c in candidates])

    # Size Filter
    valid_indices = []
    min_mult = CONFIG["filtering_thresholds"]["size_multiplier_min"]
    max_mult = CONFIG["filtering_thresholds"]["size_multiplier_max"]
    for c in candidates:
        if min_mult * local_median_area < c["area"] < max_mult * local_median_area:
            valid_indices.append(c["index"])

    if valid_indices:
        filtered_res = {
            "masks": torch.from_numpy(masks[valid_indices]),
            "boxes": torch.from_numpy(boxes[valid_indices]),
            "scores": torch.from_numpy(scores[valid_indices]),
        }
        plot_results(
            Image.fromarray(item["original_image"]), 
            filtered_res, 
            save_path=os.path.join(CONFIG["paths"]["output_dir"], f"filtered_{os.path.basename(item['path'])}")
        )
